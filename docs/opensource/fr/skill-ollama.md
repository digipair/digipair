# @digipair/skill-ollama

**Version:** 0.1.0  
**R√©sum√©:** Communication avec un serveur Ollama  
**Description:** Ex√©cution de mod√®les LLM via un serveur Ollama.  
**Ic√¥ne:** üöÄ

## Table des mati√®res

- [Fonctions](#fonctions)
  - [model](#model)
  - [embeddings](#embeddings)

---

## Fonctions

### model

Ex√©cute un mod√®le LLM depuis un serveur Ollama.

#### Param√®tres

| Nom         | Type    | Requis | Description                                              |
|-------------|---------|--------|----------------------------------------------------------|
| model       | string  | Oui    | Nom du mod√®le LLM √† charger                              |
| temperature | number  | Non    | Temp√©rature du mod√®le LLM                                |
| baseUrl     | string  | Non    | Adresse du serveur Ollama                                |
| format      | string  | Non    | Format de sortie des donn√©es g√©n√©r√©es par le mod√®le      |

#### Exemple d'utilisation

```js
const result = await model({
  model: "llama2",
  temperature: 0.7,
  baseUrl: "http://localhost:11434",
  format: "json"
});
```

#### Exemple d'appel JSON

```json
{
  "library": "@digipair/skill-ollama",
  "element": "model",
  "properties": {
    "model": "llama2",
    "temperature": 0.7,
    "baseUrl": "http://localhost:11434",
    "format": "json"
  }
}
```

---

### embeddings

Ex√©cute un mod√®le d'embeddings depuis un serveur Ollama.

#### Param√®tres

| Nom         | Type    | Requis | Description                                              |
|-------------|---------|--------|----------------------------------------------------------|
| model       | string  | Oui    | Nom du mod√®le d'embeddings √† charger                     |
| temperature | number  | Non    | Temp√©rature du mod√®le d'embeddings                       |
| baseUrl     | string  | Non    | Adresse du serveur Ollama                                |

#### Exemple d'utilisation

```js
const result = await embeddings({
  model: "nomic-embed-text",
  temperature: 0.5,
  baseUrl: "http://localhost:11434"
});
```

#### Exemple d'appel JSON

```json
{
  "library": "@digipair/skill-ollama",
  "element": "embeddings",
  "properties": {
    "model": "nomic-embed-text",
    "temperature": 0.5,
    "baseUrl": "http://localhost:11434"
  }
}
```

---

## Notes

- Les fonctions `model` et `embeddings` permettent d'interagir avec un serveur Ollama pour ex√©cuter respectivement des mod√®les LLM et des mod√®les d'embeddings.
- Le param√®tre `model` est obligatoire pour chaque fonction et doit correspondre au nom du mod√®le disponible sur le serveur Ollama.
- Le param√®tre `baseUrl` permet de sp√©cifier l'adresse du serveur Ollama si elle diff√®re de la valeur par d√©faut.
- Le param√®tre `temperature` ajuste la cr√©ativit√© des r√©ponses g√©n√©r√©es par le mod√®le (plus la valeur est √©lev√©e, plus les r√©ponses sont vari√©es).
- Le param√®tre `format` (pour la fonction `model`) permet de d√©finir le format de sortie souhait√© (par exemple : `json`, `text`, etc.).

---

**Contact & Support**  
Pour toute question ou contribution, veuillez consulter le d√©p√¥t GitHub du projet ou contacter l'√©quipe de d√©veloppement.