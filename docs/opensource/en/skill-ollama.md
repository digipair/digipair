# @digipair/skill-ollama

**Version:** 0.1.0  
**Summary:** Communication with an Ollama server  
**Description:** Run LLM models via an Ollama server.  
**Icon:** ðŸš€

## Table of Contents

- [Functions](#functions)
  - [model](#model)
  - [embeddings](#embeddings)

---

## Functions

### model

Runs an LLM model from an Ollama server.

#### Parameters

| Name        | Type   | Required | Description                                       |
| ----------- | ------ | -------- | ------------------------------------------------- |
| model       | string | Yes      | Name of the LLM model to load                     |
| temperature | number | No       | Temperature for the LLM model                     |
| baseUrl     | string | No       | Ollama server address                             |
| format      | string | No       | Output format for the data generated by the model |

#### Usage Example

```js
const result = await model({
  model: 'llama2',
  temperature: 0.7,
  baseUrl: 'http://localhost:11434',
  format: 'json',
});
```

#### JSON Call Example

```json
{
  "library": "@digipair/skill-ollama",
  "element": "model",
  "properties": {
    "model": "llama2",
    "temperature": 0.7,
    "baseUrl": "http://localhost:11434",
    "format": "json"
  }
}
```

---

### embeddings

Runs an embeddings model from an Ollama server.

#### Parameters

| Name        | Type   | Required | Description                          |
| ----------- | ------ | -------- | ------------------------------------ |
| model       | string | Yes      | Name of the embeddings model to load |
| temperature | number | No       | Temperature for the embeddings model |
| baseUrl     | string | No       | Ollama server address                |

#### Usage Example

```js
const result = await embeddings({
  model: 'nomic-embed-text',
  temperature: 0.5,
  baseUrl: 'http://localhost:11434',
});
```

#### JSON Call Example

```json
{
  "library": "@digipair/skill-ollama",
  "element": "embeddings",
  "properties": {
    "model": "nomic-embed-text",
    "temperature": 0.5,
    "baseUrl": "http://localhost:11434"
  }
}
```

---

## Notes

- The `model` and `embeddings` functions allow you to interact with an Ollama server to run LLM and embeddings models, respectively.
- The `model` parameter is required for each function and must match the name of a model available on the Ollama server.
- The `baseUrl` parameter allows you to specify the Ollama server address if it differs from the default value.
- The `temperature` parameter adjusts the creativity of the model's responses (the higher the value, the more varied the responses).
- The `format` parameter (for the `model` function) allows you to define the desired output format (e.g., `json`, `text`, etc.).

---

**Contact & Support**  
For any questions or contributions, please refer to the project's GitHub repository or contact the development team.
