{
  "openapi": "3.0.0",
  "info": {
    "title": "@digipair/skill-ollama",
    "summary": "Communication avec un serveur Ollama",
    "description": "Ex√©cution des models LLM via un serveur Ollama.",
    "version": "0.1.0",
    "x-icon": "üöÄ"
  },
  "paths": {
    "/model": {
      "post": {
        "tags": ["service"],
        "summary": "Mod√®le LLM Ollama",
        "description": "Ex√©cution d'un mod√®le LLM depuis un serveur Ollama",
        "parameters": [
          {
            "name": "model",
            "summary": "Nom du model",
            "required": true,
            "description": "Nom du model LLM √† charger",
            "schema": {
              "type": "string"
            }
          },
          {
            "name": "temperature",
            "summary": "Temperature",
            "required": false,
            "description": "Temperature du model LLM",
            "schema": {
              "type": "number"
            }
          },
          {
            "name": "baseUrl",
            "summary": "Adresse du serveur",
            "required": false,
            "description": "Adresse du serveur Ollama",
            "schema": {
              "type": "string"
            }
          },
          {
            "name": "format",
            "summary": "Format de sortie",
            "required": false,
            "description": "Format de sortie des donn√©es g√©n√©r√©es par le model",
            "schema": {
              "type": "string"
            }
          }
        ],
        "x-events": [],
        "responses": {
          "200": {
            "description": "Instance de mod√®le LLM Ollama cr√©√©e avec succ√®s",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "description": "Instance de mod√®le LangChain ChatOllama pr√™te pour la conversation",
                  "properties": {
                    "model": {
                      "type": "string",
                      "description": "Nom du mod√®le (ex: mistral, llama2, codellama)"
                    },
                    "temperature": {
                      "type": "number",
                      "description": "Param√®tre de temp√©rature du mod√®le"
                    },
                    "baseUrl": {
                      "type": "string",
                      "description": "URL du serveur Ollama"
                    },
                    "keepAlive": {
                      "type": "string",
                      "description": "Dur√©e de maintien du mod√®le actif"
                    }
                  }
                }
              }
            }
          },
          "400": {
            "description": "Requ√™te invalide - Nom de mod√®le ou param√®tres invalides"
          },
          "503": {
            "description": "Service indisponible - Serveur Ollama non accessible"
          },
          "500": {
            "description": "Erreur serveur - Erreur de chargement ou d'initialisation du mod√®le"
          }
        }
      }
    },
    "/embeddings": {
      "post": {
        "tags": ["service"],
        "summary": "Mod√®le d'embeddings Ollama",
        "description": "Ex√©cution d'un mod√®le d'enbeddings depuis un serveur Ollama",
        "parameters": [
          {
            "name": "model",
            "summary": "Nom du model",
            "required": true,
            "description": "Nom du model d'enbeddings √† charger",
            "schema": {
              "type": "string"
            }
          },
          {
            "name": "temperature",
            "summary": "Temperature",
            "required": false,
            "description": "Temperature du model d'emdeddings",
            "schema": {
              "type": "number"
            }
          },
          {
            "name": "baseUrl",
            "summary": "Adresse du serveur",
            "required": false,
            "description": "Adresse du serveur Ollama",
            "schema": {
              "type": "string"
            }
          }
        ],
        "x-events": [],
        "responses": {
          "200": {
            "description": "Instance de mod√®le d'embeddings Ollama cr√©√©e avec succ√®s",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "description": "Instance de mod√®le LangChain OllamaEmbeddings pr√™te pour l'embedding de texte",
                  "properties": {
                    "model": {
                      "type": "string",
                      "description": "Nom du mod√®le d'embeddings (ex: nomic-embed-text, all-minilm)"
                    },
                    "baseUrl": {
                      "type": "string",
                      "description": "URL du serveur Ollama"
                    },
                    "dimensions": {
                      "type": "number",
                      "description": "Dimensions du vecteur d'embedding"
                    }
                  }
                }
              }
            }
          },
          "400": {
            "description": "Requ√™te invalide - Nom de mod√®le d'embeddings ou param√®tres invalides"
          },
          "503": {
            "description": "Service indisponible - Serveur Ollama non accessible"
          },
          "500": {
            "description": "Erreur serveur - Erreur de chargement ou d'initialisation du mod√®le d'embeddings"
          }
        }
      }
    }
  },
  "components": {
    "schemas": {}
  }
}
